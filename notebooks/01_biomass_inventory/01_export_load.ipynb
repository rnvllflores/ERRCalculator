{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess ODK data to organized tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Google Cloud Imports\n",
    "import pandas_gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util imports\n",
    "sys.path.append(\"../../\")  # include parent directory\n",
    "from src.settings import DATA_DIR, GCP_PROJ_ID\n",
    "from src.biomass_inventory import (\n",
    "    extract_trees,\n",
    "    extract_stumps,\n",
    "    extract_dead_trees_class1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "URL = \"https://api.ona.io/api/v1/data/763932.csv\"\n",
    "FILE_RAW = DATA_DIR / \"csv\" / \"biomass_inventory_raw.csv\"\n",
    "CARBON_POOLS_OUTDIR = DATA_DIR / \"csv\" / \"carbon_pools\"\n",
    "NESTS = [2, 3, 4]\n",
    "\n",
    "# BigQuery Variables\n",
    "DATASET_ID = \"biomass_inventory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "CARBON_POOLS_OUTDIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data from ONA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = {\n",
    "    col: str\n",
    "    for col in (\n",
    "        28,\n",
    "        399,\n",
    "        400,\n",
    "        407,\n",
    "        408,\n",
    "        415,\n",
    "        416,\n",
    "        845,\n",
    "        846,\n",
    "        853,\n",
    "        854,\n",
    "        861,\n",
    "        862,\n",
    "        869,\n",
    "        870,\n",
    "        877,\n",
    "        878,\n",
    "        885,\n",
    "        886,\n",
    "        893,\n",
    "        894,\n",
    "        901,\n",
    "        902,\n",
    "        909,\n",
    "        910,\n",
    "        1179,\n",
    "        1180,\n",
    "        1187,\n",
    "        1188,\n",
    "        1195,\n",
    "        1196,\n",
    "        1203,\n",
    "        1204,\n",
    "        1211,\n",
    "        1212,\n",
    "        1219,\n",
    "        1220,\n",
    "        1286,\n",
    "        1337,\n",
    "        1342,\n",
    "        1347,\n",
    "        1352,\n",
    "        1357,\n",
    "        1362,\n",
    "        1378,\n",
    "        1392,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FILE_RAW.exists():\n",
    "    data = pd.read_csv(FILE_RAW, dtype=column_types)\n",
    "else:\n",
    "    urllib.request.urlretrieve(URL, FILE_RAW)\n",
    "    data = pd.read_csv(FILE_RAW, dtype=column_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with \"1\" for Primary and \"2\" for Backup\n",
    "data[\"plot_info/plot_type_short\"] = data[\"plot_info/plot_type\"].apply(\n",
    "    lambda x: \"1\" if x == \"Primary\" else \"2\"\n",
    ")\n",
    "\n",
    "# Extract subplot letters (assuming they are included in the 'plot_info.sub_plot' column)\n",
    "data[\"subplot_letter\"] = data[\"plot_info/sub_plot\"].str.replace(\"sub_plot\", \"\")\n",
    "\n",
    "# Create the unique ID by concatenating the specified columns\n",
    "data[\"unique_id\"] = (\n",
    "    data[\"plot_info/plot_code_nmbr\"].astype(str)\n",
    "    + data[\"subplot_letter\"]\n",
    "    + data[\"plot_info/plot_type_short\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Plot info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_info_cols = [\n",
    "    \"unique_id\",\n",
    "    \"plot_info/data_recorder\",\n",
    "    \"plot_info/team_no\",\n",
    "    \"plot_info/plot_code_nmbr\",\n",
    "    \"plot_info/plot_type\",\n",
    "    \"plot_info/sub_plot\",\n",
    "    \"plot_info/yes_no\",\n",
    "    \"plot_shift/sub_plot_shift\",\n",
    "    \"plot_GPS/GPS_waypt\",\n",
    "    \"plot_GPS/GPS_id\",\n",
    "    \"plot_GPS/GPS\",\n",
    "    \"plot_GPS/_GPS_latitude\",\n",
    "    \"plot_GPS/_GPS_longitude\",\n",
    "    \"plot_GPS/_GPS_altitude\",\n",
    "    \"plot_GPS/_GPS_precision\",\n",
    "    \"plot_GPS/photo\",\n",
    "    \"access/access_reason/slope\",\n",
    "    \"access/access_reason/danger\",\n",
    "    \"access/access_reason/distance\",\n",
    "    \"access/access_reason/water\",\n",
    "    \"access/access_reason/prohibited\",\n",
    "    \"access/access_reason/other\",\n",
    "    \"access/manual_reason\",\n",
    "    \"lc_data/lc_type\",\n",
    "    \"lc_class/lc_class\",\n",
    "    \"lc_class/lc_class_other\",\n",
    "    \"disturbance/disturbance_yesno\",\n",
    "    \"disturbance_data/disturbance_type\",\n",
    "    \"disturbance_class/disturbance_class\",\n",
    "    \"slope/slope\",\n",
    "    \"canopy/avg_height\",\n",
    "    \"canopy/can_cov\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_info = data[plot_info_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_info.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract info per carbon pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Living Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = extract_trees(data, NESTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees.info(), trees.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data and upload to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "trees.to_csv(CARBON_POOLS_OUTDIR / \"trees.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to BQ\n",
    "pandas_gbq.to_gbq(trees, f\"{DATASET_ID}.trees\", project_id=GCP_PROJ_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note (delete when addressed): removed `'biomass_per_kg_tree': [biomass_per_kg_tree],`. In the original code there was a placeholder column created, this can be added later in the process when biomass per tree is actually calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stumps = extract_stumps(data, NESTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stumps.info(), stumps.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data and upload to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "stumps.to_csv(CARBON_POOLS_OUTDIR / \"stumps.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to BQ\n",
    "pandas_gbq.to_gbq(stumps, f\"{DATASET_ID}.stumps\", project_id=GCP_PROJ_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dead Trees: Class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_trees_c1 = extract_dead_trees_class1(data, NESTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   unique_id     2 non-null      object \n",
      " 1   nest          2 non-null      int64  \n",
      " 2   species_name  2 non-null      float64\n",
      " 3   DBH_cl1       2 non-null      float64\n",
      " 4   class         2 non-null      int64  \n",
      " 5   subclass      2 non-null      object \n",
      "dtypes: float64(2), int64(2), object(2)\n",
      "memory usage: 228.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "   unique_id  nest  species_name  DBH_cl1  class subclass\n",
       " 0     290C2     3         145.0     38.0      1      n/a\n",
       " 1     290C2     4         177.0     58.8      1      n/a)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dead_trees_c1.info(), dead_trees_c1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data and upload to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_trees_c1.to_csv(CARBON_POOLS_OUTDIR / \"dead_trees_class1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to BQ\n",
    "pandas_gbq.to_gbq(dead_trees_c1, f\"{DATASET_ID}.dead_trees_c1\", project_id=GCP_PROJ_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dead Trees: Class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No dead trees of class 2 found in nest 2\n",
      "No dead trees of class 2 found in nest 3\n",
      "No dead trees of class 2 found in nest 4\n"
     ]
    }
   ],
   "source": [
    "dead_trees_c2 = extract_dead_trees_class2(data, NESTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " Empty DataFrame\n",
       " Columns: []\n",
       " Index: [])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dead_trees_c2.info(), dead_trees_c2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data and upload to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export CSV\n",
    "if len(dead_trees_c2) != 0:\n",
    "    dead_trees_c2.to_csv(CARBON_POOLS_OUTDIR / \"dead_trees_class2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to BQ\n",
    "if len(dead_trees_c2) != 0:\n",
    "    pandas_gbq.to_gbq(\n",
    "        dead_trees_c2, f\"{DATASET_ID}.dead_trees_c2\", project_id=GCP_PROJ_ID\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dead Trees: Tall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dead_trees_for_nests(data, nest_numbers):\n",
    "    all_dead_trees = pd.DataFrame()\n",
    "\n",
    "    for nest_number in nest_numbers:\n",
    "        # Define column name patterns\n",
    "        species_name_columns = [\n",
    "            col\n",
    "            for col in data.columns\n",
    "            if f\"tree_data_nest{nest_number}/*t_species_name_nest{nest_number}\" in col\n",
    "        ]\n",
    "        family_name_columns = [\n",
    "            col\n",
    "            for col in data.columns\n",
    "            if f\"tree_data_nest{nest_number}/*t_family_name_nest{nest_number}\" in col\n",
    "        ]\n",
    "        livedead_columns = [\n",
    "            col\n",
    "            for col in data.columns\n",
    "            if f\"tree_data_nest{nest_number}/*t_livedead_nest{nest_number}\" in col\n",
    "        ]\n",
    "        class_columns = [\n",
    "            col\n",
    "            for col in data.columns\n",
    "            if f\"tree_data_nest{nest_number}/*t_deadcl_nest{nest_number}\" in col\n",
    "        ]\n",
    "        subclass_columns = [\n",
    "            col\n",
    "            for col in data.columns\n",
    "            if f\"tree_data_nest{nest_number}/*cl2_tallshort/t_deadcl2_nest{nest_number}_tallshort\"\n",
    "            in col\n",
    "        ]\n",
    "        tall_columns = [\n",
    "            col\n",
    "            for col in data.columns\n",
    "            if f\"tree_data_nest{nest_number}/*cl2_tall/t_dead_nest{nest_number}_/*\"\n",
    "            in col\n",
    "        ]\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            for j, species_name_col in enumerate(species_name_columns):\n",
    "                if (\n",
    "                    not pd.isna(data.loc[i, livedead_columns[j]])\n",
    "                    and not pd.isna(data.loc[i, class_columns[j]])\n",
    "                    and not pd.isna(data.loc[i, subclass_columns[j]])\n",
    "                ):\n",
    "                    if (\n",
    "                        data.loc[i, livedead_columns[j]] == 2\n",
    "                        and data.loc[i, class_columns[j]] == 2\n",
    "                        and data.loc[i, subclass_columns[j]] == 2\n",
    "                    ):\n",
    "                        # Extract relevant data\n",
    "                        unique_ID = data.loc[i, \"unique_ID\"]\n",
    "                        species_name = data.loc[i, species_name_columns[j]]\n",
    "                        family_name = data.loc[i, family_name_columns[j]]\n",
    "\n",
    "                        # Extract relevant tall columns dynamically\n",
    "                        relevant_tall_columns = [\n",
    "                            col\n",
    "                            for col in tall_columns\n",
    "                            if f\"tree_data_nest{nest_number}.tree_data_nest{nest_number}_rep.{j}..\"\n",
    "                            in col\n",
    "                        ]\n",
    "                        tall_data = data.loc[i, relevant_tall_columns]\n",
    "\n",
    "                        # Rename the columns to ensure consistency (convert to lowercase)\n",
    "                        tall_data.columns = [\n",
    "                            col.lower().replace(f\".*t_dead_nest{nest_number}_\", \"\")\n",
    "                            for col in tall_data.columns\n",
    "                        ]\n",
    "\n",
    "                        # Combine all data into a single row\n",
    "                        new_row = pd.DataFrame(\n",
    "                            {\n",
    "                                \"unique_ID\": [unique_ID],\n",
    "                                \"nest\": [nest_number],\n",
    "                                \"species_name\": [species_name],\n",
    "                                \"family_name\": [family_name],\n",
    "                                **tall_data.to_dict(orient=\"list\"),\n",
    "                                \"class\": [2],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        # Append the new row to the result data frame\n",
    "                        all_dead_trees = pd.concat(\n",
    "                            [all_dead_trees, new_row], ignore_index=True\n",
    "                        )\n",
    "\n",
    "    return all_dead_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_number = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_name_columns = [\n",
    "    col\n",
    "    for col in data.columns\n",
    "    if f\"tree_data_nest{nest_number}/*t_species_name_nest{nest_number}\" in col\n",
    "]\n",
    "family_name_columns = [\n",
    "    col\n",
    "    for col in data.columns\n",
    "    if f\"tree_data_nest{nest_number}/*t_family_name_nest{nest_number}\" in col\n",
    "]\n",
    "livedead_columns = [\n",
    "    col\n",
    "    for col in data.columns\n",
    "    if f\"tree_data_nest{nest_number}/*t_livedead_nest{nest_number}\" in col\n",
    "]\n",
    "class_columns = [\n",
    "    col\n",
    "    for col in data.columns\n",
    "    if f\"tree_data_nest{nest_number}/*t_deadcl_nest{nest_number}\" in col\n",
    "]\n",
    "subclass_columns = [\n",
    "    col\n",
    "    for col in data.columns\n",
    "    if f\"tree_data_nest{nest_number}/*cl2_tallshort/t_deadcl2_nest{nest_number}_tallshort\"\n",
    "    in col\n",
    "]\n",
    "tall_columns = [\n",
    "    col\n",
    "    for col in data.columns\n",
    "    if f\"tree_data_nest{nest_number}/*cl2_tall/t_dead_nest{nest_number}_/*\" in col\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species_name_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onebase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
